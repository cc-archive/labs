---
layout: post
title: Converting a remote, headless server to RAID1
date: 2012-04-06 17:36:33.000000000 -04:00
categories: []
tags:
- debian
- system administration
status: publish
type: post
published: true
meta:
  _edit_last: '11'
author:
  login: nkinkade
  email: ''
  display_name: nkinkade
  first_name: ''
  last_name: ''
---
<p>We have a particular server which has been running very well for the past few years, but I have had a certain amount of low-level anxiety that the hard disk in the machine might fail, causing the sites it hosts to go down.  We have backups, of course, but restoring a machine from backups would take hours, if for no other reason than transferring gigabytes of data to the restored disk.  So I had our hosting company add a second hard disk to the machine so that I could attempt to covert it to boot to a RAID1 array.  Playing around with how a system boots on a remote, headless machine to which you have no console access is a little nerve-racking, because it's very easy to make a small mistake and have the machine fail to boot.  You are then at the mercy of a data center technician, and the machine may be down for some time.</p>
<p>There are several documents on the Web to be found about how to go about doing this, but I found <a href="http://www.howtoforge.com/software-raid1-grub-boot-debian-etch">one in particular</a> to be the most useful.  I pretty much followed the instructions in that document line for line, until it broke down because that person was on a Debian Etch system running GRUB version 1, but our system is running Debian Squeeze with GRUB 2 (1.98).  The steps I followed differ from those in that document starting on <a href="http://www.howtoforge.com/software-raid1-grub-boot-debian-etch-p2">page two</a> around half way down where the person says "<em>Now up to the GRUB boot loader.</em>".  Here are the steps I used to configure GRUB 2:</p>
<p>The instructions below assume the following.  Things may be different on your system, so you will have to change device names to match those on your system:</p>
<pre>
Current system:

/dev/sda1 = /boot
/dev/sd5 = swap
/dev/sd6 = /

New RAID1 arrays:

/dev/md0 = /boot
/dev/md1 = swap
/dev/md2 = /
</pre>
<p>Create a file called <em>/etc/grub.d/06_raid</em> with the following contents.  Be sure to make the file executable:</p>
<pre>
#!/bin/sh
exec tail -n +3 $0
# A custom file for CC to get system to boot to RAID1 array

menuentry "Debian GNU/Linux, with Linux 2.6.32-5-amd64, RAID1"
{
        insmod raid
        insmod mdraid
        insmod ext2
	set root='(md0)'
	search --no-floppy --fs-uuid --set 075a7fbf-eed4-4903-8988-983079658873
	echo 'Loading Linux 2.6.32-5-amd64 with RAID1 ...'
	linux /vmlinuz-2.6.32-5-amd64 root=/dev/md2 ro quiet panic=5
	echo 'Loading initial ramdisk ...'
	initrd  /initrd.img-2.6.32-5-amd64
}
</pre>
<p>Of course, you are going to need to change the UUID in the <em>search</em> line, and also the kernel and initrd image names to match those on your system, and probably some other details.</p>
<p>We should tell GRUB to fall back on a known good boot scenario, just in case something doesn't work.  I will say up front that this <strong>completely saved my ass</strong>, since it took me numerous reboots before I found a working configuration, and if it weren't for this fall-back mechanism the machine would have stuck on a failed boot screen in GRUB.  I found <a href="http://translate.google.com/translate?sl=auto&tl=en&js=n&prev=_t&hl=en&ie=UTF-8&layout=2&eotf=1&u=http%3A%2F%2Fwiki.ubuntuusers.de%2FGRUB_2%2FSkripte%23Fallback-Skript">some instructions</a> about how to go about doing this in GRUB 2.  Create a file name <em>/etc/grub.d/01_fallback</em> and add the following contents.  Be sure to make the file executable:</p>
<pre>
#! /bin/sh -e

if [ ! "x${GRUB_DEFAULT}" = "xsaved"  ] ; then
  if [ "x${GRUB_FALLBACK}" = "x" ] ; then 
      export  GRUB_FALLBACK = ""
     GRUB_FALLBACK = $( ls /boot | grep -c 'initrd.img' )
     [ ${ GRUB_DISABLE_LINUX_RECOVERY } ] || GRUB_FALLBACK = $(( ${ GRUB_FALLBACK } * 2 ))
  fi
   echo "fallback set to menuentry=${GRUB_FALLBACK}"  >&2

  cat << EOF
  set fallback="${GRUB_FALLBACK}"
EOF

fi
</pre>
<p>Then add the following line to <em>/etc/default/grub</em>:</p>
<pre>export GRUB_FALLBACK="1"</pre>
<p>And while you are in there, also uncomment or add the following line, unless you plan to be using UUIDs everywhere.  I'm not sure if this is necessary, but since I was mostly using device names (e.g. <em>/dev/md0</em>) everywhere, I figured it couldn't hurt.</p>
<pre>GRUB_DISABLE_LINUX_UUID=true</pre>
<p>Update the GRUB configuration by executing the following command:</p>
<pre># update-grub</pre>
<p>Make sure GRUB is installed and configured in the MBR of each of our disks:</p>
<pre>
# grub-install /dev/sda
# grub-install /dev/sdb
</pre>
<p>Now we need to update the initramfs image so that it knows about our RAID set up.  You could do this by simply running <em>update-initramfs -u</em>, but I found that running the following command did this for me, and perhaps some other relevant things(?), and also it verified that my mdadm settings were where they needed to be:</p>
<pre># dpkg-reconfigure mdadm</pre>
<p>I used <em>rsync</em>, instead of <em>cp</em>, to move the data from the running system to the degraded arrays like so:</p>
<pre>
# rsync -ax /boot/ /mnt/md0
# rsync -ax / /mnt/md2
</pre>
<p>When rsync finishes moving <em>/</em> to <em>/mnt/md2</em>, then edit the following files, chaning any references to the current disk to our new mdX devices:</p>
<pre>
# vi /mnt/md2/etc/fstab
# vi /mnt/md2/etc/mtab
</pre>
<p><strong>Warning</strong>: do not edit <em>/etc/fstab</em> and <em>/etc/mtab</em> on the currently running system, as the howtoforge.com instructions would seem to indicate, else if the new RAID configuration fails and the machine has to fall back to the current system, then it won't be able to boot that either.</p>
<p>I believe that was it, though it's possible I may have forgot to add a step here.  Don't run the following command unless you can afford to possibly have the machine down for a while. This is a good time to also make sure you have good backups. But if you're ready, then run:</p>
<pre># shutdown -rf now</pre>
<p>Now cross your fingers and hope the system comes back up on the RAID1 arrays, or at all.</p>
<p>If the machine comes back up on the RAID1 arrays, then you can now add the original disk to the new arrays with commands like the following:</p>
<pre>
# mdadm --manage /dev/md0 --add /dev/sda1
# mdadm --manage /dev/md1 --add /dev/sda5
# mdadm --manage /dev/md2 --add /dev/sda6
</pre>
<p>The arrays will automatically rebuild themselves, and you can check the status by running:</p>
<pre>#cat /proc/mdstat</pre>
